{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06-线性回归之梯度下降-python实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3)\n",
      "(10000,)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = 2 * np.random.random(size=20000).reshape(-1, 2)\n",
    "y = X[:, 0] * 2. + X[:, 1] * 3. + 5. + np.random.normal(size=10000)\n",
    "temp = np.ones((len(y), 1))\n",
    "X_b = np.hstack((X,temp))                                              #为了矩阵运算方便在X中加上全为1的一列\n",
    "theta = np.zeros(X_b.shape[1])                                          #theta是参数，梯度下降通过不断更新theta的值使损失函数达到最小值\n",
    "eta = 0.01                                                              #eta代表是学习速率\n",
    "episilon = 1e-8                                                         #episilon用来判断损失函数是否收敛\n",
    "print(X_b.shape)\n",
    "print(y.shape)\n",
    "print(theta.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 批量梯度下降法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J(theta, X_b, y): \n",
    "    '''\n",
    "    损失函数\n",
    "    '''\n",
    "    return np.sum((y - np.dot(X_b, theta))**2) / len(y)\n",
    "\n",
    "def dJ(theta, X_b, y):\n",
    "    '''\n",
    "    损失函数对theta的偏导数\n",
    "    '''\n",
    "    gradient = X_b.T.dot(X_b.dot(theta) - y) * 2. / len(y)\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_decent(theta, X_b, y):\n",
    "    '''\n",
    "    梯度下降过程\n",
    "    '''\n",
    "    while True:\n",
    "        last_theta = theta\n",
    "        theta = theta - eta * dJ(theta, X_b, y)\n",
    "        if abs(J(theta, X_b, y) - J(last_theta, X_b, y)) <= episilon:  #判断损失函数是否收敛，也可以限定最大迭代次数\n",
    "            break\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.00405871 2.98056298 5.00839877]\n"
     ]
    }
   ],
   "source": [
    "rst = gradient_decent(theta, X_b, y)\n",
    "print(rst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机梯度下降法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.00024478 3.01304123 4.99249275]\n"
     ]
    }
   ],
   "source": [
    "def dJ_sgd(theta, X_b_i, y_i):\n",
    "    return X_b_i.T.dot(X_b_i.dot(theta) - y_i) * 2\n",
    "\n",
    "def sgd(X_b_i, y, theta, n_iters):\n",
    "    t0 = 5\n",
    "    t1 = 50\n",
    "    \n",
    "    def learn_rate(t):\n",
    "        return t0/(t + t1)\n",
    "    \n",
    "    theta = theta\n",
    "    for cur_iter in range(n_iters):\n",
    "        rand_i = np.random.randint(len(X_b))\n",
    "        gradient = dJ_sgd(theta, X_b[rand_i], y[rand_i])\n",
    "        theta = theta - learn_rate(cur_iter) * gradient\n",
    "    \n",
    "    return theta\n",
    "print(sgd(X_b, y, theta, n_iters=len(X_b)//3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小批量随机下降\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.00226962 2.98841039 5.02916189]\n"
     ]
    }
   ],
   "source": [
    "def dJ_mbgd(theta, X_b_n, y_n, num):\n",
    "    return X_b_n.T.dot(X_b_n.dot(theta) - y_n) * 2 / num\n",
    "\n",
    "def mbgd(theta, X_b, y, num, n_iters):\n",
    "    t0 = 5\n",
    "    t1 = 50\n",
    "    theta = theta\n",
    "    num = num\n",
    "    \n",
    "    def learn_rate(t):\n",
    "        return t0/(t + t1)\n",
    "    \n",
    "    for cur_iter in range(n_iters):\n",
    "        x_index = np.random.randint(0, len(y), num)\n",
    "        gradient = dJ_mbgd(theta, X_b[x_index,], y[x_index], num)\n",
    "        theta  = theta -  learn_rate(cur_iter) * gradient\n",
    "        \n",
    "    return theta\n",
    "print(mbgd(theta, X_b, y, num=20, n_iters=len(X_b)//3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
